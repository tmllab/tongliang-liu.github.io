
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="shortcut icon" href="paper_dragon.ico">
    <link rel="stylesheet" type="text/css" href="style.css" />
    <!-- <link rel="shortcut icon" href="paper_dragon.ico"> -->
    <title>Tongliang Liu's Homepage </title>
    <base href="https://tongliang-liu.github.io/index.html">
    <!-- <base href="./index.html"> -->
    
    <script type="text/javascript" src="//ra.revolvermaps.com/0/0/8.js?i=0gh03rwju0o&amp;m=0&amp;s=170&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33&amp;v0=80" async="async"></script>
    </head>

<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<h1 style="padding-left: 0.5em">Tongliang Liu (Assistant Professor at USYD)</h1><hr>
<td id="layout-menu">
    <div class="menu-item"><a href="index.html">Home</a></div>
    <div class="menu-item"><a href="groups.html">Group</a></div>
    <div class="menu-item"><a href="talks.html">Talks</a></div>
    <div class="menu-item"><a href="teaching.html">Teaching</a></div>
    <div class="menu-item"><a href="service.html">Service</a></div>
    <div class="menu-item"><a href="research.html">Research</a></div>
    <div class="menu-item"><a href="publications.html">Publications</a></div>
    <div class="menu-item"><a href="code.html">Codes & Data</a></div>
    <div class="menu-item"><a href="award.html">Awards & honours</a></div>
</td>
<td id="layout-content">

    <h1 style="margin-top: 0em">Selected Research Topics</h1><br>
        <h2><hr><a name="news"></a>Learning with noisy labels</h2>
        Learning with noisy labels becomes a more and more important topic recently. The reason is that, in the era of big data, datasets are becoming larger and larger. Often, large-scale datasets are infeasible to be annotated accurately due to the cost and time, which naturally brings us cheap datasets with noisy labels. However, the noisy dataset can severally degenerate the performance of machine learning models, especially for the deep neural networks, as they easily memorize and eventually fit label noise. Normally, there are two ways to deal with label noise. One is to extract confident examples, whose labels are correct with a high probability. Another one is to model the noise and then get rid of the side-effect of label noise, i.e., obtain the optimal classifier defined by the clean data by exploiting the noisy data. 
   <h3>Relevant Work/Publications:</h3>
  <ul>
         <li><p>Learning under instance-dependent label noise [<a href="">ICML’20, <a href="">NeurIPS’20</a>, <a href="">AAAI’21</a>, <a href="">CVPR'21</a>]</p></li>
        <li><p>Estimate the label noise transition matrix by using anchor points [<a href="https://arxiv.org/pdf/1411.7718.pdf">TPAMI’15</a>, <a href="https://proceedings.neurips.cc/paper/2020/file/512c5cad6c37edb98ae91c8a76c3a291-Paper.pdf">NeurIPS’20</a>]</p></li>
        <li><p>Estimate the label noise transition matrix without using anchor points [<a href="">CVPR’18</a>, <a href="">NeurIPS’19</a>]</p></li>
        <li><p>Deep representation learning under label noise [<a href="">ICML’20, <a href="">ICML’20</a>]</p></li>
        <li><p>Learning with complementary labels [<a href="">ECCV’18, <a href="">AAAI’20</a>]</p></li>
        <li><p>Learning with group noise [<a href="">AAAI’21</a>]</p></li>
        <li><p>Harnessing side information for classification under label noise [<a href="">TNNLS’20</a>]</p></li>
        <li><p>Dealing with label noise in the face recognition problem [<a href="">ECCV’20</a>]</p></li>
       </ul>
     <div>
       


</td>
</tr>
</table>
</body>
</html>
